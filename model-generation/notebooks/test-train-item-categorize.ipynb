{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = tf.keras.utils.get_file('hyad.csv', 'file:///tf/notebooks/hyad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        Item Category Subcategory\n",
       "0  Store Brand Chunky Cinnamon Bread, 1 each   Bakery       Bread\n",
       "1            Store Brand French Bread, 16 oz   Bakery       Bread\n",
       "2              Store Brand Rye Bread, 1 each   Bakery       Bread\n",
       "3            Store Brand Vienna Bread, 16 oz   Bakery       Bread\n",
       "4             Store Brand Wheat Bread, 16 oz   Bakery       Bread"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Item</th>\n      <th>Category</th>\n      <th>Subcategory</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Store Brand Chunky Cinnamon Bread, 1 each</td>\n      <td>Bakery</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Store Brand French Bread, 16 oz</td>\n      <td>Bakery</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Store Brand Rye Bread, 1 each</td>\n      <td>Bakery</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Store Brand Vienna Bread, 16 oz</td>\n      <td>Bakery</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Store Brand Wheat Bread, 16 oz</td>\n      <td>Bakery</td>\n      <td>Bread</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a CsvDataset to create the dataset without creating this all on the file system\n",
    "# as the example shows\n",
    "tf_csv = tf.data.experimental.CsvDataset(\n",
    "    ['file:///tf/notebooks/hyad.csv'], \n",
    "    record_defaults=[\"\", \"\", \"\"], \n",
    "    compression_type=None, \n",
    "    buffer_size=None, \n",
    "    header=True, \n",
    "    field_delim=',',\n",
    "    use_quote_delim=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: I need to assign numbers for each unique category, then store it for safe keeping\n",
    "\n",
    "# get all unique categories\n",
    "ALL_CATEGORIES_SUB_CATEGORIES = set([])\n",
    "for element in tf_csv.as_numpy_iterator():\n",
    "    ALL_CATEGORIES_SUB_CATEGORIES.add(f\"{element[1].decode('utf-8')}|{element[2].decode('utf-8')}\")\n",
    "\n",
    "# now create a dict of String to Int\n",
    "ALL_CAT_SUB_CAT_TO_INT = { cat: i for (i, cat) in enumerate(ALL_CATEGORIES_SUB_CATEGORIES)}\n",
    "\n",
    "# pprint.pprint(ALL_CAT_SUB_CAT_TO_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Max label value: 221\n"
     ]
    }
   ],
   "source": [
    "# convert tf_csv to a numpy \n",
    "numpy_slices = list(tf_csv.as_numpy_iterator())\n",
    "\n",
    "def fix(x):\n",
    "    key = \"|\".join([x[1].decode('utf-8'), x[2].decode('utf-8')])\n",
    "    return ( x[0], ALL_CAT_SUB_CAT_TO_INT[key] )\n",
    "# map this into a new numpy_slices with the numeric code for the category\n",
    "new_numpy_slices = list(map(fix , numpy_slices))\n",
    "\n",
    "text_list = list(map(lambda x: x[0], new_numpy_slices))\n",
    "label_list = list(map(lambda x: x[1], new_numpy_slices))\n",
    "\n",
    "label_max_val = max(label_list)\n",
    "print(f\"Max label value: {label_max_val}\")\n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_list)\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "zipped_ds = tf.data.Dataset.zip((text_ds, label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we want to shuffle the dataset so that we have a good batch of different items\n",
    "# we'll shuffle using a seed of 3, and every time repeat() is run we'll get another shuffle\n",
    "# tf_csv = tf_csv.shuffle(3, reshuffle_each_iteration=True)\n",
    "\n",
    "# create our train, validate, and test buckets\n",
    "train_ds = zipped_ds.batch(500).take(320)\n",
    "validate_ds = zipped_ds.batch(500).skip(320).take(80)\n",
    "test_ds = zipped_ds.batch(500).skip(400).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Item:  b'Store Brand Chunky Cinnamon Bread, 1 each'\nCategory ID:  15\nItem:  b'Store Brand French Bread, 16 oz'\nCategory ID:  15\nItem:  b'Store Brand Rye Bread, 1 each'\nCategory ID:  15\nItem:  b'Store Brand Vienna Bread, 16 oz'\nCategory ID:  15\nItem:  b'Store Brand Wheat Bread, 16 oz'\nCategory ID:  15\nItem:  b'Store Brand White Bread, 1 each'\nCategory ID:  15\nItem:  b'Store Brand Shells 4 cnt Dessert, 3.25 oz'\nCategory ID:  212\nItem:  b'Store Brand Angel Food Cake, 13 oz'\nCategory ID:  212\nItem:  b'Store Brand Brownies, 1 each'\nCategory ID:  212\nItem:  b'Store Brand Cookies Chocolate Chip 12 cnt, 1 each'\nCategory ID:  212\n"
     ]
    }
   ],
   "source": [
    "for item_batch, cat_id_batch in train_ds.take(1):\n",
    "    for i in range(10):\n",
    "        print(\"Item: \", item_batch.numpy()[i])\n",
    "        print(\"Category ID: \", cat_id_batch.numpy()[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 1 - use a bag-of-words model\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# we create a binary layer based on the vocab\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')\n",
    "\n",
    "# we create another layer based on the sequence length\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the layers for just category\n",
    "train_text = train_ds.take(1).map(lambda item, cat_id: item)\n",
    "\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_text(text, cat):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return binary_vectorize_layer(text), cat\n",
    "\n",
    "def int_vectorize_text(text, cat):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return int_vectorize_layer(text), cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Item tf.Tensor(b'Store Brand Chunky Cinnamon Bread, 1 each', shape=(), dtype=string)\nCategory tf.Tensor(15, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, cat_batch = next(iter(train_ds))\n",
    "first_item, first_cat = text_batch[0], cat_batch[0]\n",
    "print(\"Item\", first_item)\n",
    "print(\"Category\", first_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'binary' vectorization label:  tf.Tensor([[0. 0. 0. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n'int' vectorization label:  tf.Tensor(\n[[  5   6 630  46  12  27  37   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]], shape=(1, 250), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"'binary' vectorization label: \", binary_vectorize_text(first_item, first_cat)[0])\n",
    "print(\"'int' vectorization label: \", int_vectorize_text(first_item, first_cat)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9 --->  juice\n",
      "3 --->  12\n",
      "Vocabulary size: 727\n"
     ]
    }
   ],
   "source": [
    "print(\"9 ---> \", int_vectorize_layer.get_vocabulary()[9])\n",
    "print(\"3 ---> \", int_vectorize_layer.get_vocabulary()[3])\n",
    "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply layers to the three buckets of data\n",
    "binary_train_ds = train_ds.map(binary_vectorize_text)\n",
    "binary_val_ds = validate_ds.map(binary_vectorize_text)\n",
    "binary_test_ds = test_ds.map(binary_vectorize_text)\n",
    "\n",
    "int_train_ds = train_ds.map(int_vectorize_text)\n",
    "int_val_ds = validate_ds.map(int_vectorize_text)\n",
    "int_test_ds = test_ds.map(int_vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance tuning\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "binary_train_ds = configure_dataset(binary_train_ds)\n",
    "binary_val_ds = configure_dataset(binary_val_ds)\n",
    "binary_test_ds = configure_dataset(binary_test_ds)\n",
    "\n",
    "int_train_ds = configure_dataset(int_train_ds)\n",
    "int_val_ds = configure_dataset(int_val_ds)\n",
    "int_test_ds = configure_dataset(int_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 5.4076 - accuracy: 0.0023\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 5.3730 - accuracy: 0.0319\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 5.3439 - accuracy: 0.1232\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 5.3155 - accuracy: 0.2568\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 5.2873 - accuracy: 0.3712\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 5.2593 - accuracy: 0.4796\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 5.2314 - accuracy: 0.5436\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 5.2036 - accuracy: 0.5821\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 5.1758 - accuracy: 0.6141\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 5.1481 - accuracy: 0.6358\n"
     ]
    }
   ],
   "source": [
    "# Train a simple bag-of-words linear model\n",
    "binary_model = tf.keras.Sequential([layers.Dense(label_max_val + 1)])\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = binary_model.fit(\n",
    "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}